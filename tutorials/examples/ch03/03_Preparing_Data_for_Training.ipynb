{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "#### By Paul Hendricks\n",
    "-------\n",
    "\n",
    "Before we build our first machine learning model, we need to structure our data in such a way that it can fed into the model. \n",
    "\n",
    "In this notebook, we will also show how to preprocess data with GPU DataFrames using cuDF in RAPIDS.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* Introduction to Data Preprocessing\n",
    "* Setup\n",
    "* cuDF fundamentals\n",
    "  * Creating our first cuDF DataFrame\n",
    "    * Using lists\n",
    "    * Using a list of tuples or a dictionary\n",
    "    * Loading from a Pandas DataFrame\n",
    "    * Loading from disk\n",
    "  * Understanding data types\n",
    "    * Accessing dtype information\n",
    "    * Modifying the dtypes of a cuDF DataFrame\n",
    "  * Working with Series\n",
    "    * To be edited.\n",
    "  * Working with Indexes\n",
    "    * To be edited.\n",
    "  * Renaming row and column names\n",
    "    * To be edited.\n",
    "  * Creating and deleting columns\n",
    "    * To be edited.\n",
    "  * Basic DataFrame operations\n",
    "    * Selecting columns\n",
    "    * Selecting rows\n",
    "    * Ordering and sorting data\n",
    "    * Removing missing values\n",
    "  * Exploring and summarizing\n",
    "* Cleaning and engineering features\n",
    "  * Working with Strings\n",
    "  * Working with Dates and DateTime data types\n",
    "    * foo\n",
    "* Transforming data with joins, concatenation, reshaping, and pivoting\n",
    "  * To be edited.\n",
    "  * One Hot Encoding\n",
    "* Aggregating data using groupbys\n",
    "* Time Series\n",
    "  * To be edited.\n",
    "* Conclusion\n",
    "\n",
    "Before going any further, let's make sure we have access to `matplotlib`, a popular Python library for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import matplotlib; print('Matplotlib Version:', matplotlib.__version__)\n",
    "except ModuleNotFoundError:\n",
    "    os.system('conda install -y matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Preprocessing\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This notebook was tested using the following Docker containers:\n",
    "\n",
    "* `rapidsai/rapidsai:0.6-cuda10.0-devel-ubuntu18.04-gcc7-py3.7` from [DockerHub](https://hub.docker.com/r/rapidsai/rapidsai)\n",
    "* `rapidsai/rapidsai-nightly:0.6-cuda10.0-devel-ubuntu18.04-gcc7-py3.7` from [DockerHub](https://hub.docker.com/r/rapidsai/rapidsai-nightly)\n",
    "\n",
    "This notebook was run on the NVIDIA Tesla V100 GPU. Please be aware that your system may be different and you may need to modify the code or install packages to run the below examples. \n",
    "\n",
    "If you think you have found a bug or an error, please file an issue here: https://github.com/rapidsai/notebooks/issues\n",
    "\n",
    "Before we begin, let's check out our hardware setup by running the `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what CUDA version we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load some helper functions from `matplotlib` and configure the Jupyter Notebook for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first cuDF DataFrame\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf; print('cuDF Version:', cudf.__version__)\n",
    "import dask_cudf; print('Dask cuDF Version:', dask_cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a cudf.DataFrame using lists\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.DataFrame()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create two columns named \"key\" and \"value\"\n",
    "df['key'] = [0, 1, 2, 3, 4]\n",
    "df['value'] = [float(i + 10) for i in range(5)]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a cudf.DataFrame using a list of tuples or a dictionary\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "ids = np.arange(5)\n",
    "t0 = datetime.strptime('2018-10-07 12:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "datetimes = [(t0+ timedelta(seconds=x)) for x in range(5)]\n",
    "dts = np.array(datetimes, dtype='datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.DataFrame([('id', ids), ('datetimes', dts)])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.DataFrame({'id': ids, 'datetimes': dts})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from a Pandas DataFrame\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame({'a': [0, 1, 2, 3],'b': [0.1, 0.2, None, 0.3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.from_pandas(pdf)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a CSV file\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv('../datasets/iris.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Parquet file\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = cudf.read_csv('../datasets/iris.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding data types\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage dataset\n",
    "\n",
    "Going forward, we'll default to using the variable `df` to represent a Dask cuDF DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -alh /datasets/rapids/mortgage/mortgage_2000_1gb/perf\n",
    "ls -alh /datasets/rapids/mortgage/mortgage_2000_1gb/acq\n",
    "head /datasets/rapids/mortgage/mortgage_2000_1gb/names.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the cluster\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask_xgboost as dxgb_gpu\n",
    "import dask\n",
    "import dask_cudf; print('Dask cuDF Version:', dask_cudf.__version__)\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import Client, wait\n",
    "import xgboost as xgb\n",
    "import cudf; print('cuDF Version:', cudf.__version__)\n",
    "from cudf.dataframe import DataFrame\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "cmd = 'hostname --all-ip-addresses'\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance dataset\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /datasets/rapids/mortgage/mortgage_2000_1gb/perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join('/', 'datasets', 'rapids', 'mortgage', 'mortgage_2000_1gb')\n",
    "\n",
    "dtypes = OrderedDict([\n",
    "        ('loan_id', 'int64'),\n",
    "        ('monthly_reporting_period', 'date'),\n",
    "        ('servicer', 'category'),\n",
    "        ('interest_rate', 'float64'),\n",
    "        ('current_actual_upb', 'float64'),\n",
    "        ('loan_age', 'float64'),\n",
    "        ('remaining_months_to_legal_maturity', 'float64'),\n",
    "        ('adj_remaining_months_to_maturity', 'float64'),\n",
    "        ('maturity_date', 'date'),\n",
    "        ('msa', 'float64'),\n",
    "        ('current_loan_delinquency_status', 'int32'),\n",
    "        ('mod_flag', 'category'),\n",
    "        ('zero_balance_code', 'category'),\n",
    "        ('zero_balance_effective_date', 'date'),\n",
    "        ('last_paid_installment_date', 'date'),\n",
    "        ('foreclosed_after', 'date'),\n",
    "        ('disposition_date', 'date'),\n",
    "        ('foreclosure_costs', 'float64'),\n",
    "        ('prop_preservation_and_repair_costs', 'float64'),\n",
    "        ('asset_recovery_costs', 'float64'),\n",
    "        ('misc_holding_expenses', 'float64'),\n",
    "        ('holding_taxes', 'float64'),\n",
    "        ('net_sale_proceeds', 'float64'),\n",
    "        ('credit_enhancement_proceeds', 'float64'),\n",
    "        ('repurchase_make_whole_proceeds', 'float64'),\n",
    "        ('other_foreclosure_proceeds', 'float64'),\n",
    "        ('non_interest_bearing_upb', 'float64'),\n",
    "        ('principal_forgiveness_upb', 'float64'),\n",
    "        ('repurchase_make_whole_proceeds_flag', 'category'),\n",
    "        ('foreclosure_principal_write_off_amount', 'float64'),\n",
    "        ('servicing_activity_indicator', 'category')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(base_path, 'perf', 'Performance_*')\n",
    "# filepath = os.path.join(base_path, 'perf', 'Performance_2000Q1.txt_0')\n",
    "perf_df = dask_cudf.read_csv(filepath, delimiter='|', \n",
    "                        names=list(dtypes.keys()), dtype=list(dtypes.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perf_df.npartitions)\n",
    "print(perf_df.divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3700 / perf_df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of rows\n",
    "number_of_rows = perf_df.map_partitions(len).compute().sum()\n",
    "print(number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /datasets/rapids/mortgage/mortgage_2000_1gb/perf/Performance_2000Q1.txt_0 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /datasets/rapids/mortgage/mortgage_2000_1gb/perf/Performance_* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset a column to a Series\n",
    "perf_df['loan_id'].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many rows in each column have actual values\n",
    "# # ideal\n",
    "# column_counts = df.count()\n",
    "# column_counts\n",
    "\n",
    "# alternative\n",
    "column_counts = []\n",
    "for column in list(perf_df.columns):\n",
    "    column_count = perf_df[column].count().compute()\n",
    "    column_counts.append((column, column_count))\n",
    "\n",
    "print(column_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, count in column_counts:\n",
    "    print(column, ':', (count / number_of_rows) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition dataset\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh /datasets/rapids/mortgage/mortgage_2000_1gb/acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = OrderedDict([\n",
    "        ('loan_id', 'int64'),\n",
    "        ('orig_channel', 'category'),\n",
    "        ('seller_name', 'category'),\n",
    "        ('orig_interest_rate', 'float64'),\n",
    "        ('orig_upb', 'int64'),\n",
    "        ('orig_loan_term', 'int64'),\n",
    "        ('orig_date', 'date'),\n",
    "        ('first_pay_date', 'date'),\n",
    "        ('orig_ltv', 'float64'),\n",
    "        ('orig_cltv', 'float64'),\n",
    "        ('num_borrowers', 'float64'),\n",
    "        ('dti', 'float64'),\n",
    "        ('borrower_credit_score', 'float64'),\n",
    "        ('first_home_buyer', 'category'),\n",
    "        ('loan_purpose', 'category'),\n",
    "        ('property_type', 'category'),\n",
    "        ('num_units', 'int64'),\n",
    "        ('occupancy_status', 'category'),\n",
    "        ('property_state', 'category'),\n",
    "        ('zip', 'int64'),\n",
    "        ('mortgage_insurance_percent', 'float64'),\n",
    "        ('product_type', 'category'),\n",
    "        ('coborrow_credit_score', 'float64'),\n",
    "        ('mortgage_insurance_type', 'float64'),\n",
    "        ('relocation_mortgage_indicator', 'category')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(base_path, 'acq', 'Acquisition_2000Q1.txt')\n",
    "# filepath = os.path.join(base_path, 'perf', 'Acquisition_')\n",
    "acq_df = dask_cudf.read_csv(filepath, delimiter='|', \n",
    "                            names=list(dtypes.keys()), dtype=list(dtypes.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acq_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of rows\n",
    "acq_df.map_partitions(len).compute().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acq_df['orig_date'] = acq_df['orig_date'].astype(np.datetime64)\n",
    "# acq_df['first_pay_date'] = acq_df['first_pay_date'].astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acq_df.drop(['orig_date', 'first_pay_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names dataset\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /datasets/rapids/mortgage/mortgage_2000_1gb/names.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(base_path, 'names.csv')\n",
    "# filepath = os.path.join(base_path, 'perf', 'Performance_2000Q1.txt_0')\n",
    "names_df = dask_cudf.read_csv(filepath, delimiter='|', \n",
    "                              names=list(dtypes.keys()), dtype=list(dtypes.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of rows\n",
    "names_df.map_partitions(len).compute().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be edited\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be edited\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to be edited\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acq_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(acq_df), type(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acq_df.drop(['orig_date', 'first_pay_date'], axis=1)\n",
    "subset_columns = [i for i in acq_df.columns if i not in ['orig_date', 'first_pay_date']]\n",
    "print(subset_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = acq_df[subset_columns].merge(names_df, how='left', on=['seller_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of rows\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dask_task(func, **kwargs):\n",
    "    task = func(**kwargs)\n",
    "    return task\n",
    "\n",
    "def process_quarter_gpu(year=2000, quarter=1, perf_file=\"\"):\n",
    "    ml_arrays = run_dask_task(delayed(run_gpu_workflow),\n",
    "                                          quarter=quarter,\n",
    "                                          year=year,\n",
    "                                          perf_file=perf_file)\n",
    "    return client.compute(ml_arrays,\n",
    "                          optimize_graph=False,\n",
    "                          fifo_timeout=\"0ms\")\n",
    "\n",
    "def null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column] = df[column].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def run_gpu_workflow(quarter=1, year=2000, perf_file=\"\", **kwargs):\n",
    "    names = gpu_load_names()\n",
    "    acq_gdf = gpu_load_acquisition_csv(acquisition_path= acq_data_path + \"/Acquisition_\"\n",
    "                                      + str(year) + \"Q\" + str(quarter) + \".txt\")\n",
    "    acq_gdf = acq_gdf.merge(names, how='left', on=['seller_name'])\n",
    "    acq_gdf.drop_column('seller_name')\n",
    "    acq_gdf['seller_name'] = acq_gdf['new']\n",
    "    acq_gdf.drop_column('new')\n",
    "    perf_df_tmp = gpu_load_performance_csv(perf_file)\n",
    "    gdf = perf_df_tmp\n",
    "    everdf = create_ever_features(gdf)\n",
    "    delinq_merge = create_delinq_features(gdf)\n",
    "    everdf = join_ever_delinq_features(everdf, delinq_merge)\n",
    "    del(delinq_merge)\n",
    "    joined_df = create_joined_df(gdf, everdf)\n",
    "    testdf = create_12_mon_features(joined_df)\n",
    "    joined_df = combine_joined_12_mon(joined_df, testdf)\n",
    "    del(testdf)\n",
    "    perf_df = final_performance_delinquency(gdf, joined_df)\n",
    "    del(gdf, joined_df)\n",
    "    final_gdf = join_perf_acq_gdfs(perf_df, acq_gdf)\n",
    "    del(perf_df)\n",
    "    del(acq_gdf)\n",
    "    final_gdf = last_mile_cleaning(final_gdf)\n",
    "    return final_gdf\n",
    "\n",
    "def gpu_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    print(performance_path)\n",
    "    \n",
    "    return cudf.read_csv(performance_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def gpu_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "    \n",
    "    print(acquisition_path)\n",
    "    \n",
    "    return cudf.read_csv(acquisition_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)\n",
    "\n",
    "def gpu_load_names(**kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    return cudf.read_csv(col_names_path, names=cols, delimiter='|', dtype=list(dtypes.values()), skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ever_features(gdf, **kwargs):\n",
    "    everdf = gdf[['loan_id', 'current_loan_delinquency_status']]\n",
    "    everdf = everdf.groupby('loan_id', method='hash').max()\n",
    "    del(gdf)\n",
    "    everdf['ever_30'] = (everdf['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    everdf['ever_90'] = (everdf['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    everdf['ever_180'] = (everdf['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    everdf.drop_column('max_current_loan_delinquency_status')\n",
    "    return everdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delinq_features(gdf, **kwargs):\n",
    "    delinq_gdf = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(gdf)\n",
    "    delinq_30 = delinq_gdf.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop_column('min_monthly_reporting_period')\n",
    "    delinq_90 = delinq_gdf.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop_column('min_monthly_reporting_period')\n",
    "    delinq_180 = delinq_gdf.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop_column('min_monthly_reporting_period')\n",
    "    del(delinq_gdf)\n",
    "    delinq_merge = delinq_30.merge(delinq_90, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge['delinquency_90'] = delinq_merge['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge = delinq_merge.merge(delinq_180, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge['delinquency_180'] = delinq_merge['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_ever_delinq_features(everdf_tmp, delinq_merge, **kwargs):\n",
    "    everdf = everdf_tmp.merge(delinq_merge, on=['loan_id'], how='left', type='hash')\n",
    "    del(everdf_tmp)\n",
    "    del(delinq_merge)\n",
    "    everdf['delinquency_30'] = everdf['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_90'] = everdf['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_180'] = everdf['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return everdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_joined_df(gdf, everdf, **kwargs):\n",
    "    test = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(gdf)\n",
    "    test['timestamp'] = test['monthly_reporting_period']\n",
    "    test.drop_column('monthly_reporting_period')\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop_column('current_loan_delinquency_status')\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop_column('current_actual_upb')\n",
    "    test['upb_12'] = test['upb_12'].fillna(999999999)\n",
    "    test['delinquency_12'] = test['delinquency_12'].fillna(-1)\n",
    "    \n",
    "    joined_df = test.merge(everdf, how='left', on=['loan_id'], type='hash')\n",
    "    del(everdf)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'] = joined_df['ever_30'].fillna(-1)\n",
    "    joined_df['ever_90'] = joined_df['ever_90'].fillna(-1)\n",
    "    joined_df['ever_180'] = joined_df['ever_180'].fillna(-1)\n",
    "    joined_df['delinquency_30'] = joined_df['delinquency_30'].fillna(-1)\n",
    "    joined_df['delinquency_90'] = joined_df['delinquency_90'].fillna(-1)\n",
    "    joined_df['delinquency_180'] = joined_df['delinquency_180'].fillna(-1)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_12_mon_features(joined_df, **kwargs):\n",
    "    testdfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        tmpdf = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        tmpdf['josh_months'] = tmpdf['timestamp_year'] * 12 + tmpdf['timestamp_month']\n",
    "        tmpdf['josh_mody_n'] = ((tmpdf['josh_months'].astype('float64') - 24000 - y) / 12).floor()\n",
    "        tmpdf = tmpdf.groupby(['loan_id', 'josh_mody_n'], method='hash').agg({'delinquency_12': 'max','upb_12': 'min'})\n",
    "        tmpdf['delinquency_12'] = (tmpdf['max_delinquency_12']>3).astype('int32')\n",
    "        tmpdf['delinquency_12'] +=(tmpdf['min_upb_12']==0).astype('int32')\n",
    "        tmpdf.drop_column('max_delinquency_12')\n",
    "        tmpdf['upb_12'] = tmpdf['min_upb_12']\n",
    "        tmpdf.drop_column('min_upb_12')\n",
    "        tmpdf['timestamp_year'] = (((tmpdf['josh_mody_n'] * n_months) + 24000 + (y - 1)) / 12).floor().astype('int16')\n",
    "        tmpdf['timestamp_month'] = np.int8(y)\n",
    "        tmpdf.drop_column('josh_mody_n')\n",
    "        testdfs.append(tmpdf)\n",
    "        del(tmpdf)\n",
    "    del(joined_df)\n",
    "\n",
    "    return cudf.concat(testdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop_column('delinquency_12')\n",
    "    joined_df.drop_column('upb_12')\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_performance_delinquency(gdf, joined_df, **kwargs):\n",
    "    merged = null_workaround(gdf)\n",
    "    joined_df = null_workaround(joined_df)\n",
    "    merged['timestamp_month'] = merged['monthly_reporting_period'].dt.month\n",
    "    merged['timestamp_month'] = merged['timestamp_month'].astype('int8')\n",
    "    merged['timestamp_year'] = merged['monthly_reporting_period'].dt.year\n",
    "    merged['timestamp_year'] = merged['timestamp_year'].astype('int16')\n",
    "    merged = merged.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "    merged.drop_column('timestamp_year')\n",
    "    merged.drop_column('timestamp_month')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_perf_acq_gdfs(perf, acq, **kwargs):\n",
    "    perf = null_workaround(perf)\n",
    "    acq = null_workaround(acq)\n",
    "    return perf.merge(acq, how='left', on=['loan_id'], type='hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    for column in drop_list:\n",
    "        df.drop_column(column)\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(-1)\n",
    "    return df.to_arrow(preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: The ETL calculates additional features which are then dropped before creating the XGBoost DMatrix.\n",
    "# This can be optimized to avoid calculating the dropped features.\n",
    "\n",
    "gpu_dfs = []\n",
    "gpu_time = 0\n",
    "quarter = 1\n",
    "year = start_year\n",
    "count = 0\n",
    "while year <= end_year:\n",
    "    for file in glob(os.path.join(perf_data_path + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\")):\n",
    "        gpu_dfs.append(process_quarter_gpu(year=year, quarter=quarter, perf_file=file))\n",
    "        count += 1\n",
    "    quarter += 1\n",
    "    if quarter == 5:\n",
    "        year += 1\n",
    "        quarter = 1\n",
    "wait(gpu_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            1,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    'objective':         'gpu:reg:linear',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gpu_dfs = [delayed(DataFrame.from_arrow)(gpu_df) for gpu_df in gpu_dfs[:part_count]]\n",
    "gpu_dfs = [gpu_df for gpu_df in gpu_dfs]\n",
    "wait(gpu_dfs)\n",
    "\n",
    "tmp_map = [(gpu_df, list(client.who_has(gpu_df).values())[0]) for gpu_df in gpu_dfs]\n",
    "new_map = {}\n",
    "for key, value in tmp_map:\n",
    "    if value not in new_map:\n",
    "        new_map[value] = [key]\n",
    "    else:\n",
    "        new_map[value].append(key)\n",
    "\n",
    "del(tmp_map)\n",
    "gpu_dfs = []\n",
    "for list_delayed in new_map.values():\n",
    "    gpu_dfs.append(delayed(cudf.concat)(list_delayed))\n",
    "\n",
    "del(new_map)\n",
    "gpu_dfs = [(gpu_df[['delinquency_12']], gpu_df[delayed(list)(gpu_df.columns.difference(['delinquency_12']))]) for gpu_df in gpu_dfs]\n",
    "gpu_dfs = [(gpu_df[0].persist(), gpu_df[1].persist()) for gpu_df in gpu_dfs]\n",
    "\n",
    "gpu_dfs = [dask.delayed(xgb.DMatrix)(gpu_df[1], gpu_df[0]) for gpu_df in gpu_dfs]\n",
    "gpu_dfs = [gpu_df.persist() for gpu_df in gpu_dfs]\n",
    "gc.collect()\n",
    "wait(gpu_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = None\n",
    "bst = dxgb_gpu.train(client, dxgb_gpu_params, gpu_dfs, labels, num_boost_round=dxgb_gpu_params['nround'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "\n",
    "To learn more about RAPIDS, be sure to check out: \n",
    "\n",
    "* [Open Source Website](http://rapids.ai)\n",
    "* [GitHub](https://github.com/rapidsai/)\n",
    "* [Press Release](https://nvidianews.nvidia.com/news/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning)\n",
    "* [NVIDIA Blog](https://blogs.nvidia.com/blog/2018/10/10/rapids-data-science-open-source-community/)\n",
    "* [Developer Blog](https://devblogs.nvidia.com/gpu-accelerated-analytics-rapids/)\n",
    "* [NVIDIA Data Science Webpage](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
